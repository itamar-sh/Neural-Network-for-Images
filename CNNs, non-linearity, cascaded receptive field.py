# -*- coding: utf-8 -*-
"""Copy of Copy of cifar10_tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1Mn_mBgLULsU3ABVA3SwBicOKMvAEUS
"""

# Commented out IPython magic to ensure Python compatibility.
# For tips on running notebooks in Google Colab, see
# https://pytorch.org/tutorials/beginner/colab
# %matplotlib inline

"""Imports"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
!pip install wandb
import wandb
wandb.login()

"""Load Data"""

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 4

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

"""Function for find test loss"""

def record_test_loss(cur_optimizer):
    test_loss = 0.0
    with torch.no_grad():
        for i, data in enumerate(testloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data[0].to(device), data[1].to(device)
            # forward + backward + optimize
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            test_loss += loss.item()
    return test_loss

"""Function to train the model, and evaluation his permoances while training and after training."""

def check_nets(nets):
    for j, net in enumerate(nets):
        wandb.init(
        # Set the project where this run will be logged
        project="cifar10_CNN_convs_15_epochs", 
        # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)
        name=net.get_name(), 
        # Track hyperparameters and run metadata
        config={
        "learning_rate": 0.001,
        "architecture": "CNN",
        "dataset": "CIFAR-50",
        "epochs": 2,
        })


        for epoch in range(15):  # loop over the dataset multiple times

            running_loss = 0.0
            for i, data in enumerate(trainloader, 0):
                # get the inputs; data is a list of [inputs, labels]
                inputs, labels = data[0].to(device), data[1].to(device)

                # zero the parameter gradients
                optimizers[j].zero_grad()

                # forward + backward + optimize
                outputs = net(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizers[j].step()

                # find test loss


                # print statistics
                running_loss += loss.item()
                # record the train and the test loss
                if i % 2000 == 1999:
                    cur_test_loss = record_test_loss(optimizers[j])
                    wandb.log({"train_loss": running_loss, "test_loss": cur_test_loss})
                    print(f'train epoch: [{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 2000:.3f} test loss: {cur_test_loss :.3f}')

                    running_loss = 0.0
                    
        print('Finished Training')

        # find the accuracy of the 3_8 net
        correct = 0
        total = 0
        # since we're not training, we don't need to calculate the gradients for our outputs
        with torch.no_grad():
            for data in testloader:
                images, labels = data[0].to(device), data[1].to(device)
                # calculate outputs by running images through the network
                outputs = net(images)
                # the class with the highest energy is what we choose as prediction
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')
        wandb.log({"test accuracy": 100 * correct // total})

"""First Question - simple nets with two conv layers, two max pools and two relu activations."""

class Net(nn.Module):
    def __init__(self, conv1_output_channels, conv2_output_channels):
        super().__init__()
        self.name = f"conv1={conv1_output_channels}, conv2={conv2_output_channels}"
        self.conv1 = nn.Conv2d(3, conv1_output_channels, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(conv1_output_channels, conv2_output_channels, 5)
        self.fc = nn.Linear(conv2_output_channels * 5 * 5, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

    def get_name(self):
        return self.name

nets = []
params = [
    [12, 32]
]
for net_params in params:
    nets.append(Net(*net_params).to(device))
for net in nets:
    print(net)
criterion = nn.CrossEntropyLoss()
optimizers = [optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for net in nets]
check_nets(nets)

"""Second Question - Non linear check without pooling at all, only two conv."""

# non linear net
class NonLinearNet(nn.Module):
    def __init__(self, conv1_output_channels, conv2_output_channels):
        super().__init__()
        self.name = f"Non Linear: conv1={conv1_output_channels}, conv2={conv2_output_channels}"
        self.conv1 = nn.Conv2d(3, conv1_output_channels, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(conv1_output_channels, conv2_output_channels, 5)
        self.fc = nn.Linear(conv2_output_channels * 24 * 24, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

    def get_name(self):
        return self.name

nets = []
params = [
    [12, 32]
]
for net_params in params:
    nets.append(NonLinearNet(*net_params).to(device))
for net in nets:
    print(net)

criterion = nn.CrossEntropyLoss()
optimizers = [optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for net in nets]
check_nets(nets)

"""Continue Second Question - Check non linear with 2 avg pool instead of 2 max pool."""

# non linear net with avg pooling
class NonLinearNetAVG(nn.Module):
    def __init__(self, conv1_output_channels, conv2_output_channels):
        super().__init__()
        self.name = f"Non Linear Avg pooling: conv1={conv1_output_channels}, conv2={conv2_output_channels}"
        self.conv1 = nn.Conv2d(3, conv1_output_channels, 5)
        self.pool = nn.AvgPool2d(2, 2)
        self.conv2 = nn.Conv2d(conv1_output_channels, conv2_output_channels, 5)
        self.fc = nn.Linear(conv2_output_channels * 5 * 5, 10)

    def forward(self, x):
        x = self.pool(self.conv1(x))
        x = self.pool(self.conv2(x))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

    def get_name(self):
        return self.name

nets = []
params = [
    [12, 32]
]
for net_params in params:
    nets.append(NonLinearNetAVG(*net_params).to(device))
for net in nets:
    print(net)

criterion = nn.CrossEntropyLoss()
optimizers = [optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for net in nets]

"""Continue Second Question - Check non linear with 4 conv layers instead of 2.

"""

# 4 layers of conv
# non linear net with avg pooling
class NonLinearDoubleConvNet(nn.Module):
    def __init__(self, conv1_output_channels, conv2_output_channels):
        super().__init__()
        self.name = f"Non Linear double conv: conv1,2={conv1_output_channels}, conv3,4={conv2_output_channels}"
        self.conv1 = nn.Conv2d(3, conv1_output_channels, 5)
        self.conv2 = nn.Conv2d(conv1_output_channels, conv1_output_channels, 5)
        self.conv3 = nn.Conv2d(conv1_output_channels, conv2_output_channels, 5)
        self.conv4 = nn.Conv2d(conv2_output_channels, conv2_output_channels, 5)
        self.fc = nn.Linear(conv2_output_channels * 16 * 16, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

    def get_name(self):
        return self.name

nets = []
params = [
    [12, 32]
]
for net_params in params:
    nets.append(NonLinearDoubleConvNet(*net_params).to(device))
for net in nets:
    print(net)

criterion = nn.CrossEntropyLoss()
optimizers = [optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for net in nets]

"""Third Question - check only one conv layer."""

class NetReceptiveField(nn.Module):
    def __init__(self, conv1_output_channels):
        super().__init__()
        self.name = f"ReceptiveFieldNet: conv1={conv1_output_channels}"
        self.conv1 = nn.Conv2d(3, conv1_output_channels, 5)
        self.fc = nn.Linear(conv1_output_channels * 28 * 28, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

    def get_name(self):
        return self.name

nets = []
params = [
    [12]
]
for net_params in params:
    nets.append(NetReceptiveField(*net_params).to(device))
for net in nets:
    print(net)
criterion = nn.CrossEntropyLoss()
optimizers = [optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for net in nets]
check_nets(nets)

wandb.finish()